{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_list = ['hokkaido','aomori','akita','yamagata','iwate','miyagi', 'fukushima',\n",
    "             'aichi','gifu','shizuoka','mie','niigata','yamanashi', 'nagano', \n",
    "             'ishikawa','toyama','fukui','okayama','hiroshima','tottori','shimane',\n",
    "             'yamaguchi','kagawa','tokushima','ehime','kochi','tokyo','kanagawa',\n",
    "             'chiba','tochigi','ibaraki','gunma','osaka','hyogo','kyoto','shiga',\n",
    "             'nara','wakayama','fukuoka','saga','nagasaki','kumamoto','oita',\n",
    "             'miyazaki','kagoshima','okinawa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pref in tqdm(pref_list):\n",
    "    \n",
    "    isContinue = True\n",
    "    num = 1\n",
    "    \n",
    "    content = []\n",
    "    while(isContinue):\n",
    "        if(num == 1):\n",
    "            print('Loading pages of '+pref)\n",
    "        \n",
    "        site_url = \"https://tabelog.com/\"+pref+\"/rstLst/\"+str(num)+\"/?Srt=D&SrtT=rvcn\"\n",
    "        nem = urllib.request.urlopen(site_url).read()\n",
    "        soup = BeautifulSoup(nem,\"html.parser\")\n",
    "        names = soup.find_all(\"a\", class_=\"list-rst__rst-name-target cpy-rst-name\")\n",
    "        rates = soup.find_all(\"span\", class_=\"c-rating__val c-rating__val--strong list-rst__rating-val\")\n",
    "        amounts = soup.find_all(\"em\", class_=\"list-rst__rvw-count-num cpy-review-count\")\n",
    "\n",
    "        amounts = [int(i.text) for i in amounts]\n",
    "\n",
    "        if(min(amounts) <= 100):\n",
    "            isContinue = False\n",
    "            idx = np.argmin(np.abs(np.array(amounts) - 100))\n",
    "        else:\n",
    "            idx = False\n",
    "\n",
    "        if(idx is not False):\n",
    "            url = [i.get('href') for i in names][:idx]\n",
    "            name = [i.text for i in names][:idx]\n",
    "            rates = [float(i.text) for i in rates][:idx]\n",
    "            amounts = amounts[:idx]\n",
    "        else:\n",
    "            url = [i.get('href') for i in names]\n",
    "            name = [i.text for i in names]\n",
    "            rates = [float(i.text) for i in rates]\n",
    "\n",
    "        for n,r,u,a in zip(name,rates,url,amounts):\n",
    "            dic = {'name': n, 'rates': r, 'amounts': a, 'url': u, 'pref':pref}\n",
    "            content.append(dic)\n",
    "\n",
    "        num += 1\n",
    "        if(num == 60):\n",
    "            isContinue = False\n",
    "        time.sleep(1)\n",
    "        \n",
    "    contents.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = np.array(contents)\n",
    "np.save('scraping.npy', contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = np.load('scraping.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,content in enumerate(tqdm(contents)):\n",
    "    for m,c in enumerate(tqdm(content)):\n",
    "        url = c['url']\n",
    "        isLoad = True\n",
    "        try:\n",
    "            nem = urllib.request.urlopen(url).read()\n",
    "        except:\n",
    "            isLoad = False\n",
    "            time.sleep(5)\n",
    "        if(isLoad == True):\n",
    "            soup = BeautifulSoup(nem,\"html.parser\")\n",
    "            isPremium = soup.find('h3', class_='pr-comment-title js-pr-title')\n",
    "            if(isPremium is None):\n",
    "                contents[n][m].update({'isPremium': False})\n",
    "            else:\n",
    "                contents[n][m].update({'isPremium': True})\n",
    "            contents[n][m].update({'isLoad': True})\n",
    "        else:\n",
    "            contents[n][m].update({'isPremium': False})\n",
    "            contents[n][m].update({'isLoad': False})\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = np.array(contents)\n",
    "np.save('total_contents.npy', contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
